{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Balancing Robot in PyBullet\n",
    "**Balance and control of a 2-wheeled robot simulated with PyBullet Physics library**\n",
    "<br>V2: Everything Implemented in GYM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet\n",
    "import time\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GYM Environment For Robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfBalancing(gym.Env):\n",
    "    #metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SelfBalancing, self).__init__()\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Box(low=0.0, high=+1.0,shape=(3,),dtype=np.float64)\n",
    "        self.observation_space = spaces.Box(low=np.array([-np.pi/2,-1000]), high=np.array([+np.pi/2,+1000]))\n",
    "        \"\"\"\n",
    "            Action Space: action[0] -> kp, action[1] -> ki, action[2] -> kd\n",
    "            Observation Space: torso_pitch orientation, torso linear speed\n",
    "        \"\"\"\n",
    "        self.state = np.array([0.0,0.0])\n",
    "        self.steps = 0\n",
    "        self.max_episode_steps = 2500\n",
    "        # Instantiate PyBullet\n",
    "        phisycsClient = pybullet.connect(pybullet.GUI)\n",
    "        pybullet.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        # Spawn Robot\n",
    "        self.robotID = None\n",
    "        self.reset()\n",
    "        # Initialize Controller Parameters\n",
    "        self.integral = 0\n",
    "        self.derivative = 0\n",
    "        self.prev_error = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        motion = self.controller(action)\n",
    "        self.take_action(motion)\n",
    "        ## Calculating reward\n",
    "        reward = self.calculate_reward()\n",
    "        obs = self.observe()\n",
    "        done = self.terminated()\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def take_action(self,motion):\n",
    "        # Takes a tuple as input\n",
    "        # motion --> (left wheel speed, right wheel speed)\n",
    "        pybullet.setJointMotorControl2(bodyUniqueId=self.robotID, \n",
    "                        jointIndex=0, \n",
    "                        controlMode=pybullet.VELOCITY_CONTROL,\n",
    "                        targetVelocity = motion[0])\n",
    "        pybullet.setJointMotorControl2(bodyUniqueId=self.robotID, \n",
    "                        jointIndex=1, \n",
    "                        controlMode=pybullet.VELOCITY_CONTROL,\n",
    "                        targetVelocity = motion[1])\n",
    "        pybullet.stepSimulation()\n",
    "        time.sleep(1.0/400)\n",
    "        self.steps += 1\n",
    "    \n",
    "    def observe(self):\n",
    "        position, orientation = pybullet.getBasePositionAndOrientation(self.robotID)\n",
    "        self.state[0] = np.array([pybullet.getEulerFromQuaternion(orientation)[0]])\n",
    "        linear_vel, anagular_vel = pybullet.getBaseVelocity(self.robotID)\n",
    "        self.state[1] = (linear_vel[0]**2 + linear_vel[1]**2 + linear_vel[2]**2) ** 0.5\n",
    "        return self.state\n",
    "    \n",
    "    def calculate_reward(self):\n",
    "        reward = - (self.observe()[0]**2 + self.observe()[1]**2)\n",
    "        if self.terminated():\n",
    "            reward += (self.steps - self.max_episode_steps) / 500\n",
    "        return reward\n",
    "    \n",
    "    def controller(self,action):\n",
    "        ## Simple PID\n",
    "        error = self.observe()[0]\n",
    "        self.integral += error\n",
    "        self.derivative = error - self.prev_error\n",
    "        self.prev_error = error\n",
    "        \n",
    "        motion = ((action[0]*1000) * error + (action[1]*0.1) * self.integral + (action[2]*100) * self.derivative)\n",
    "        return (motion,motion)\n",
    "    \n",
    "    def reset(self):\n",
    "        pybullet.resetSimulation()\n",
    "        planeID = pybullet.loadURDF(\"plane.urdf\")\n",
    "        pybullet.setGravity(0,0,-9.81)\n",
    "        theta = np.random.uniform(high = np.pi/4 , low = -np.pi/4)\n",
    "        self.robotID = pybullet.loadURDF(\"robot.urdf\",\n",
    "                                 [0.0,0.0,0.0],pybullet.getQuaternionFromEuler([theta,0.0,0.0]),useFixedBase = 0)\n",
    "        pybullet.setRealTimeSimulation(0) # change to (1) for real time simulation\n",
    "        self.state = self.observe()\n",
    "        self.steps = 0\n",
    "        return self.state\n",
    "    \n",
    "    def terminated(self):\n",
    "        ## If the robot tilt angle reaches 75 degrees or\n",
    "        ## the simulation reaches its maximum time steps\n",
    "        if self.steps > self.max_episode_steps or abs(self.state[0]) > (np.pi / (75/80)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __del__(self):\n",
    "        pybullet.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Environment with (Kp = 650, Ki = 0.005, Kd = 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SelfBalancing()\n",
    "\n",
    "while not env.terminated():\n",
    "    env.step((0.55,0.1,0.25))\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantagous Actor Critic (A2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kassra/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "[a2c|MainThread|absl|WARNING] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/kassra/.local/lib/python3.6/site-packages/jax/_src/numpy/lax_numpy.py:2842: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"asarray\")\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 1,\tT: 2,501,\tG: -3.3,\tavg_G: 0,\tt: 2500,\tdt: 11.936ms,\tSimpleTD/loss: 0.000429,\tVanillaPG/loss: -0.000461\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 2,\tT: 5,002,\tG: -5.56e+04,\tavg_G: 0,\tt: 2500,\tdt: 10.514ms,\tSimpleTD/loss: 47.3,\tVanillaPG/loss: -0.588\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 3,\tT: 7,503,\tG: -789,\tavg_G: 0,\tt: 2500,\tdt: 10.631ms,\tSimpleTD/loss: 10.6,\tVanillaPG/loss: 1.5\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 4,\tT: 10,004,\tG: -1.65e+05,\tavg_G: 0,\tt: 2500,\tdt: 10.596ms,\tSimpleTD/loss: 144,\tVanillaPG/loss: -0.368\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 5,\tT: 12,505,\tG: -3.44e+03,\tavg_G: 0,\tt: 2500,\tdt: 10.449ms,\tSimpleTD/loss: 7.13,\tVanillaPG/loss: 0.322\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 6,\tT: 15,006,\tG: -2.58e+03,\tavg_G: 0,\tt: 2500,\tdt: 10.568ms,\tSimpleTD/loss: 6.97,\tVanillaPG/loss: -0.668\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 7,\tT: 17,507,\tG: -2.98e+03,\tavg_G: 0,\tt: 2500,\tdt: 10.629ms,\tSimpleTD/loss: 4.08,\tVanillaPG/loss: 0.589\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 8,\tT: 20,008,\tG: -5.34e+03,\tavg_G: 0,\tt: 2500,\tdt: 10.561ms,\tSimpleTD/loss: 5.46,\tVanillaPG/loss: -0.607\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 9,\tT: 22,509,\tG: -7.26e+04,\tavg_G: 0,\tt: 2500,\tdt: 10.430ms,\tSimpleTD/loss: 66,\tVanillaPG/loss: -0.372\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import coax\n",
    "import optax\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from numpy import prod\n",
    "\n",
    "# pick environment\n",
    "name = 'a2c'\n",
    "env = SelfBalancing()\n",
    "env = coax.wrappers.TrainMonitor(env, name=name, tensorboard_dir=f\"./data/tensorboard/{name}\") \n",
    "\n",
    "def func_v(S, is_training):\n",
    "    # custom haiku function\n",
    "    value = hk.Sequential([\n",
    "                          hk.Linear(20),\n",
    "                          hk.Linear(20),\n",
    "                          hk.Linear(1,w_init=jnp.zeros),jnp.ravel])\n",
    "    return value(S)  # output shape: (batch_size,)\n",
    "\n",
    "def func_pi(S, is_training):\n",
    "    shared = hk.Sequential((\n",
    "        hk.Linear(20), jax.nn.relu,\n",
    "        hk.Linear(20), jax.nn.relu,\n",
    "    ))\n",
    "    mu = hk.Sequential((\n",
    "        shared,\n",
    "        hk.Linear(10), jax.nn.relu,\n",
    "        hk.Linear(3, w_init=jnp.zeros),\n",
    "        hk.Reshape(env.action_space.shape),\n",
    "    ))\n",
    "    logvar = hk.Sequential((\n",
    "        shared,\n",
    "        hk.Linear(8), jax.nn.relu,\n",
    "        hk.Linear(3, w_init=jnp.zeros),\n",
    "        hk.Reshape(env.action_space.shape),\n",
    "    ))\n",
    "    return {'mu': mu(S), 'logvar': logvar(S)}\n",
    "\n",
    "# function approximators\n",
    "v = coax.V(func_v, env)\n",
    "pi = coax.Policy(func_pi, env)\n",
    "\n",
    "\n",
    "# specify how to update policy and value function\n",
    "vanilla_pg = coax.policy_objectives.VanillaPG(pi, optimizer=optax.adam(0.001))\n",
    "simple_td = coax.td_learning.SimpleTD(v, optimizer=optax.adam(0.002))\n",
    "\n",
    "\n",
    "# specify how to trace the transitions\n",
    "tracer = coax.reward_tracing.NStep(n=5, gamma=0.9)\n",
    "buffer = coax.experience_replay.SimpleReplayBuffer(capacity=256)\n",
    "\n",
    "\n",
    "for ep in range(10):\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(env.max_episode_steps):\n",
    "        a, logp = pi(s, return_logp=True)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "\n",
    "        # add transition to buffer\n",
    "        # N.B. vanilla-pg doesn't use logp but we include it to make it easy to\n",
    "        # swap in another policy updater that does require it, e.g. ppo-clip\n",
    "        tracer.add(s, a, r, done, logp)\n",
    "        while tracer:\n",
    "            buffer.add(tracer.pop())\n",
    "\n",
    "        # update\n",
    "        if len(buffer) == buffer.capacity:\n",
    "            for _ in range(4 * buffer.capacity // 32):  # ~4 passes\n",
    "                transition_batch = buffer.sample(batch_size=32)\n",
    "                metrics_v, td_error = simple_td.update(transition_batch, return_td_error=True)\n",
    "                metrics_pi = vanilla_pg.update(transition_batch, td_error)\n",
    "                env.record_metrics(metrics_v)\n",
    "                env.record_metrics(metrics_pi)\n",
    "\n",
    "            buffer.clear()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        s = s_next\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[a2c|MainThread|TrainMonitor|INFO] ep: 1,\tT: 2,501,\tG: -60.4,\tavg_G: 0,\tt: 2500,\tdt: 11.208ms\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 2,\tT: 5,002,\tG: -2.49e+05,\tavg_G: 0,\tt: 2500,\tdt: 10.309ms\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 3,\tT: 7,503,\tG: -1.36e+03,\tavg_G: 0,\tt: 2500,\tdt: 10.710ms\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 4,\tT: 10,004,\tG: -507,\tavg_G: 0,\tt: 2500,\tdt: 10.337ms\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 5,\tT: 12,505,\tG: -113,\tavg_G: 0,\tt: 2500,\tdt: 10.174ms\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 6,\tT: 15,006,\tG: -57.8,\tavg_G: 0,\tt: 2500,\tdt: 10.293ms\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 7,\tT: 17,507,\tG: -228,\tavg_G: 0,\tt: 2500,\tdt: 10.219ms\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 8,\tT: 20,008,\tG: -1.16e+03,\tavg_G: 0,\tt: 2500,\tdt: 10.239ms\n",
      "[a2c|MainThread|TrainMonitor|INFO] ep: 9,\tT: 22,509,\tG: -5.37,\tavg_G: 0,\tt: 2500,\tdt: 10.162ms\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import coax\n",
    "import optax\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from numpy import prod\n",
    "\n",
    "# pick environment\n",
    "name = 'PPO'\n",
    "env = SelfBalancing()\n",
    "env = coax.wrappers.TrainMonitor(env, name=name, tensorboard_dir=f\"./data/tensorboard/{name}\") \n",
    "\n",
    "\n",
    "def func_v(S, is_training):\n",
    "    # custom haiku function\n",
    "    value = hk.Sequential([\n",
    "                          hk.Linear(20),\n",
    "                          hk.Linear(20),\n",
    "                          hk.Linear(1,w_init=jnp.zeros),jnp.ravel])\n",
    "    return value(S)  # output shape: (batch_size,)\n",
    "\n",
    "def func_pi(S, is_training):\n",
    "    shared = hk.Sequential((\n",
    "        hk.Linear(20), jax.nn.relu,\n",
    "        hk.Linear(20), jax.nn.relu,\n",
    "    ))\n",
    "    mu = hk.Sequential((\n",
    "        shared,\n",
    "        hk.Linear(3, w_init=jnp.zeros),\n",
    "        hk.Reshape(env.action_space.shape), jax.nn.sigmoid,\n",
    "    ))\n",
    "    logvar = hk.Sequential((\n",
    "        shared,\n",
    "        hk.Linear(3, w_init=jnp.zeros),\n",
    "         hk.Reshape(env.action_space.shape), jax.nn.sigmoid\n",
    "    ))\n",
    "    return {'mu': mu(S), 'logvar': logvar(S)}\n",
    "\n",
    "# function approximators\n",
    "v = coax.V(func_v, env)\n",
    "pi = coax.Policy(func_pi, env)\n",
    "\n",
    "\n",
    "# slow-moving avg of pi\n",
    "pi_behavior = pi.copy()\n",
    "\n",
    "\n",
    "# specify how to update policy and value function\n",
    "ppo_clip = coax.policy_objectives.PPOClip(pi, optimizer=optax.adam(0.001))\n",
    "simple_td = coax.td_learning.SimpleTD(v, optimizer=optax.adam(0.001))\n",
    "\n",
    "\n",
    "# specify how to trace the transitions\n",
    "tracer = coax.reward_tracing.NStep(n=5, gamma=0.9)\n",
    "buffer = coax.experience_replay.SimpleReplayBuffer(capacity=256)\n",
    "\n",
    "\n",
    "for ep in range(10):\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(env.max_episode_steps):\n",
    "        a, logp = pi_behavior(s, return_logp=True)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "\n",
    "        # add transition to buffer\n",
    "        tracer.add(s, a, r, done, logp)\n",
    "        while tracer:\n",
    "            buffer.add(tracer.pop())\n",
    "\n",
    "        # update\n",
    "        if len(buffer) == buffer.capacity:\n",
    "            for _ in range(4 * buffer.capacity // 32):  # ~4 passes\n",
    "                transition_batch = buffer.sample(batch_size=32)\n",
    "                metrics_v, td_error = simple_td.update(transition_batch, return_td_error=True)\n",
    "                metrics_pi = ppo_clip.update(transition_batch, td_error)\n",
    "                #env.record_metrics(metrics_v)\n",
    "                #env.record_metrics(metrics_pi)\n",
    "\n",
    "            buffer.clear()\n",
    "            pi_behavior.soft_update(pi, tau=0.1)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        s = s_next\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6141107  0.57537735 0.61373246]\n"
     ]
    }
   ],
   "source": [
    "env = SelfBalancing()\n",
    "coeff = list('')\n",
    "for i in np.arange(-np.pi/2,np.pi/2,0.1):\n",
    "    for j in range(-20,20):\n",
    "        coeff.append(pi(np.array([i,j]),return_logp=False))\n",
    "print(np.mean(np.array(coeff),axis=0))\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4427186  0.26619488 0.54129535]\n"
     ]
    }
   ],
   "source": [
    "del env\n",
    "env = SelfBalancing()\n",
    "while not env.terminated():\n",
    "    s = env.observe()\n",
    "    action = pi(s, return_logp=False)\n",
    "    env.step(action)\n",
    "del env\n",
    "print (action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Kp</td>\n",
    "        <td>Ki</td>\n",
    "        <td>Kd</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>PPO</td>\n",
    "        <td>614.11</td>\n",
    "        <td>0.026</td>\n",
    "        <td>54.13</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>A2C</td>\n",
    "        <td>524.35</td>\n",
    "        <td>0.049</td>\n",
    "        <td>51.20</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
