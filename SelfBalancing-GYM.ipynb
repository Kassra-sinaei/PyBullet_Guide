{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Balancing Robot in PyBullet\n",
    "**Balance and control of a 2-wheeled robot simulated with PyBullet Physics library**\n",
    "<br>V2: Everything Implemented in GYM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet\n",
    "import time\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GYM Environment For Robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function SelfBalancing.__del__ at 0x000002247C4F7CA8>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-fcaadefef76d>\", line 96, in __del__\n",
      "pybullet.error: Not connected to physics server.\n"
     ]
    }
   ],
   "source": [
    "class SelfBalancing(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SelfBalancing, self).__init__()\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Box(low=np.array([-1000,-0.01,-20]), high=np.array([+1000,+0.01,20]), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=np.array([-np.pi/2,-1000]), high=np.array([+np.pi/2,+1000]), dtype=np.float32)\n",
    "        \"\"\"\n",
    "            Action Space: action[0] -> kp, action[1] -> ki, action[2] -> kd\n",
    "            Observation Space: torso_pitch orientation, torso linear speed\n",
    "        \"\"\"\n",
    "        self.state = np.array([0.0,0.0])\n",
    "        self.steps = 0\n",
    "        self.max_episode_steps = 5000\n",
    "        # Instantiate PyBullet\n",
    "        phisycsClient = pybullet.connect(pybullet.GUI)\n",
    "        pybullet.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        # Spawn Robot\n",
    "        self.robotID = None\n",
    "        self.reset()\n",
    "        # Initialize Controller Parameters\n",
    "        self.integral = 0\n",
    "        self.derivative = 0\n",
    "        self.prev_error = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        motion = self.controller(action)\n",
    "        self.take_action(motion)\n",
    "        ## Calculating reward\n",
    "        reward = self.calculate_reward()\n",
    "        obs = self.observe\n",
    "        done = self.terminated()\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def take_action(self,motion):\n",
    "        # Takes a tuple as input\n",
    "        # motion --> (left wheel speed, right wheel speed)\n",
    "        pybullet.setJointMotorControl2(bodyUniqueId=self.robotID, \n",
    "                        jointIndex=0, \n",
    "                        controlMode=pybullet.VELOCITY_CONTROL,\n",
    "                        targetVelocity = motion[0])\n",
    "        pybullet.setJointMotorControl2(bodyUniqueId=self.robotID, \n",
    "                        jointIndex=1, \n",
    "                        controlMode=pybullet.VELOCITY_CONTROL,\n",
    "                        targetVelocity = motion[1])\n",
    "        pybullet.stepSimulation()\n",
    "        time.sleep(1.0/240.0)\n",
    "        self.steps += 1\n",
    "    \n",
    "    def observe(self):\n",
    "        position, orientation = pybullet.getBasePositionAndOrientation(self.robotID)\n",
    "        self.state[0] = np.array([pybullet.getEulerFromQuaternion(orientation)[0]])\n",
    "        linear_vel, anagular_vel = pybullet.getBaseVelocity(self.robotID)\n",
    "        self.state[1] = (linear_vel[0]**2 + linear_vel[1]**2 + linear_vel[2]**2) ** 0.5\n",
    "        return self.state\n",
    "    \n",
    "    def calculate_reward(self):\n",
    "        reward = - (self.observe()[0]**2 + self.observe()[1]**2)\n",
    "        if self.terminated():\n",
    "            reward += (self.steps - self.max_episode_steps) / 500\n",
    "        return reward\n",
    "    \n",
    "    def controller(self,action):\n",
    "        ## Simple PID\n",
    "        error = self.observe()[0]\n",
    "        self.integral += error\n",
    "        self.derivative = error - self.prev_error\n",
    "        self.prev_error = error\n",
    "        \n",
    "        motion = (action[0] * error + action[1] * self.integral + action[2] * self.derivative)\n",
    "        return (motion,motion)\n",
    "    \n",
    "    def reset(self):\n",
    "        pybullet.resetSimulation()\n",
    "        planeID = pybullet.loadURDF(\"plane.urdf\")\n",
    "        pybullet.setGravity(0,0,-9.81)\n",
    "        self.robotID = pybullet.loadURDF(\"robot.urdf\",\n",
    "                                 [0.0,0.0,0.0],pybullet.getQuaternionFromEuler([0.0,0.0,0.0]),useFixedBase = 0)\n",
    "        pybullet.setRealTimeSimulation(0) # change to (1) for real time simulation\n",
    "        self.state = self.observe()\n",
    "    \n",
    "    def terminated(self):\n",
    "        ## If the robot tilt angle reaches 75 degrees or\n",
    "        ## the simulation reaches its maximum time steps\n",
    "        if self.steps > self.max_episode_steps or abs(self.state[0]) > (np.pi / (75/80)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def __del__(self):\n",
    "        pybullet.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Environment with (Kp = 650, Ki = 0.005, Kd = 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SelfBalancing()\n",
    "while not env.terminated():\n",
    "    env.step((650.0,0.005,10.0))\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using COAX to Train an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import coax\n",
    "import optax\n",
    "import haiku as hk\n",
    "from jax.nn import relu\n",
    "\n",
    "# pick environment\n",
    "env = SelfBalancing()\n",
    "env = coax.wrappers.TrainMonitor(env)\n",
    "\n",
    "def func_v(S, is_training):\n",
    "    # custom haiku function\n",
    "    value = hk.Sequential([\n",
    "                          hk.Linear(20), relu,\n",
    "                          hk.Linear(20), relu,\n",
    "                          hk.Linear(1),])\n",
    "    return value(S)  # output shape: (batch_size,)\n",
    "\n",
    "def func_pi(S, is_training):\n",
    "    # custom haiku function (for discrete actions in this example)\n",
    "    logits = hk.Sequential([\n",
    "                          hk.Linear(20), relu,\n",
    "                          hk.Linear(20), relu,\n",
    "                          hk.Linear(3),])\n",
    "    return {'logits': logits(S)}  # logits shape: (batch_size, num_actions)\n",
    "\n",
    "# function approximators\n",
    "v = coax.V(func_v, env)\n",
    "pi = coax.Policy(func_pi, env)\n",
    "\n",
    "# slow-moving avg of pi\n",
    "pi_behavior = pi.copy()\n",
    "\n",
    "# specify how to update policy and value function\n",
    "ppo_clip = coax.policy_objectives.PPOClip(pi, optimizer=optax.adam(0.001))\n",
    "simple_td = coax.td_learning.SimpleTD(v, optimizer=optax.adam(0.001))\n",
    "\n",
    "\n",
    "# specify how to trace the transitions\n",
    "tracer = coax.reward_tracing.NStep(n=5, gamma=0.9)\n",
    "buffer = coax.experience_replay.SimpleReplayBuffer(capacity=256)\n",
    "\n",
    "for ep in range(100):\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        a, logp = pi_behavior(s, return_logp=True)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "\n",
    "        # add transition to buffer\n",
    "        tracer.add(s, a, r, done, logp)\n",
    "        while tracer:\n",
    "            buffer.add(tracer.pop())\n",
    "\n",
    "        # update\n",
    "        if len(buffer) == buffer.capacity:\n",
    "            for _ in range(4 * buffer.capacity // 32):  # ~4 passes\n",
    "                transition_batch = buffer.sample(batch_size=32)\n",
    "                metrics_v, td_error = simple_td.update(transition_batch, return_td_error=True)\n",
    "                metrics_pi = ppo_clip.update(transition_batch, td_error)\n",
    "                env.record_metrics(metrics_v)\n",
    "                env.record_metrics(metrics_pi)\n",
    "\n",
    "            buffer.clear()\n",
    "            pi_behavior.soft_update(pi, tau=0.1)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        s = s_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SelfBalancing()\n",
    "while not env.terminated():\n",
    "    s = env.observe()\n",
    "    action = pi(s, return_logp=False)\n",
    "    env.step(action)\n",
    "del env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
